{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6515ed6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-09-27T08:48:17.684229Z",
     "iopub.status.busy": "2022-09-27T08:48:17.683404Z",
     "iopub.status.idle": "2022-09-27T08:48:22.549729Z",
     "shell.execute_reply": "2022-09-27T08:48:22.548748Z"
    },
    "papermill": {
     "duration": 4.874973,
     "end_time": "2022-09-27T08:48:22.552252",
     "exception": false,
     "start_time": "2022-09-27T08:48:17.677279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://github.com/BeneKenobi/kaggle-titanic\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import tensorflow as tf\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "keras = tf.keras\n",
    "\n",
    "CUD_COLORS = (\n",
    "    \"#e69f00\",  # orange\n",
    "    \"#56b4e9\",  # sky-blue\n",
    "    \"#009e73\",  # bluish-green\n",
    "    \"#f0e442\",  # yellow\n",
    "    \"#0072b2\",  # blue\n",
    "    \"#d55e00\",  # vermilion\n",
    "    \"#cc79a7\",  # reddish-purple\n",
    ")\n",
    "\n",
    "\n",
    "def strip_title(name: str) -> str:\n",
    "    return name.split(\",\")[1].split(\".\")[0].strip().lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "194e84a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T08:48:22.560663Z",
     "iopub.status.busy": "2022-09-27T08:48:22.559525Z",
     "iopub.status.idle": "2022-09-27T08:48:22.594863Z",
     "shell.execute_reply": "2022-09-27T08:48:22.593995Z"
    },
    "papermill": {
     "duration": 0.040656,
     "end_time": "2022-09-27T08:48:22.596794",
     "exception": false,
     "start_time": "2022-09-27T08:48:22.556138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
    "X_test = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n",
    "passenger_ids_test = X_test[\"PassengerId\"].copy()\n",
    "Y = X[\"Survived\"].copy()\n",
    "X.drop(\"Survived\", axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c407aa8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T08:48:22.602981Z",
     "iopub.status.busy": "2022-09-27T08:48:22.602125Z",
     "iopub.status.idle": "2022-09-27T08:48:22.665695Z",
     "shell.execute_reply": "2022-09-27T08:48:22.664646Z"
    },
    "papermill": {
     "duration": 0.07013,
     "end_time": "2022-09-27T08:48:22.669117",
     "exception": false,
     "start_time": "2022-09-27T08:48:22.598987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:16: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:16: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def engineer_features(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_in.copy()\n",
    "    df[\"Title\"] = df[\"Name\"].apply(strip_title)\n",
    "    df[\"Age\"] = df.groupby([\"Sex\", \"Pclass\"])[\"Age\"].apply(\n",
    "        lambda x: x.fillna(x.median())\n",
    "    )\n",
    "    df[\"Embarked\"] = df[\"Embarked\"].fillna(\n",
    "        \"S\"\n",
    "    )  # https://www.encyclopedia-titanica.org/titanic-survivor/martha-evelyn-stone.html\n",
    "    df[\"Fare\"] = df[\"Fare\"].fillna(\n",
    "        df.groupby([\"Pclass\", \"Parch\", \"SibSp\"])[\"Fare\"].median()[3][0][0]\n",
    "    )  # Filling the missing value in Fare with the median Fare of 3rd class alone passenger\n",
    "    df[\"Deck\"] = df[\"Cabin\"].fillna(\"M\").apply(lambda x: x[0])\n",
    "    df[\"Ticket_Frequency\"] = df.groupby(\"Ticket\")[\"Ticket\"].transform(\"count\")\n",
    "    df.drop([\"Ticket\", \"PassengerId\", \"Name\", \"Cabin\"], axis=1, inplace=True)\n",
    "    df = df.fillna(df.median())\n",
    "    return df\n",
    "\n",
    "\n",
    "X = engineer_features(X)\n",
    "X_test = engineer_features(X_test)\n",
    "\n",
    "encoding_help = pd.concat([X, X_test]).copy()\n",
    "\n",
    "for column in encoding_help.columns:\n",
    "    encoder = LabelEncoder().fit(encoding_help[column])\n",
    "    X[column] = encoder.transform(X[column])\n",
    "    X_test[column] = encoder.transform(X_test[column])\n",
    "\n",
    "    encoding_help[column] = encoder.transform(encoding_help[column])\n",
    "\n",
    "scaler = StandardScaler().fit(encoding_help)\n",
    "X = scaler.transform(X)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60856250",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T08:48:22.675353Z",
     "iopub.status.busy": "2022-09-27T08:48:22.674537Z",
     "iopub.status.idle": "2022-09-27T08:48:26.434168Z",
     "shell.execute_reply": "2022-09-27T08:48:26.433198Z"
    },
    "papermill": {
     "duration": 3.764786,
     "end_time": "2022-09-27T08:48:26.436300",
     "exception": false,
     "start_time": "2022-09-27T08:48:22.671514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 08:48:23.655490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-27 08:48:23.765352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-27 08:48:23.766138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-27 08:48:23.767741: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-27 08:48:23.768070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-27 08:48:23.768780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-27 08:48:23.769432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-27 08:48:26.030499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-27 08:48:26.031292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-27 08:48:26.032046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-27 08:48:26.032657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15401 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(9, input_shape=(10,), activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c05933b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T08:48:26.443199Z",
     "iopub.status.busy": "2022-09-27T08:48:26.442575Z",
     "iopub.status.idle": "2022-09-27T08:48:53.219712Z",
     "shell.execute_reply": "2022-09-27T08:48:53.218578Z"
    },
    "papermill": {
     "duration": 26.782957,
     "end_time": "2022-09-27T08:48:53.222006",
     "exception": false,
     "start_time": "2022-09-27T08:48:26.439049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 08:48:26.512482: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "26/26 [==============================] - 2s 8ms/step - loss: 0.6997 - accuracy: 0.5069 - val_loss: 0.6879 - val_accuracy: 0.5222\n",
      "Epoch 2/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.6566 - accuracy: 0.5730 - val_loss: 0.6448 - val_accuracy: 0.6444\n",
      "Epoch 3/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.6275 - accuracy: 0.6629 - val_loss: 0.6173 - val_accuracy: 0.7111\n",
      "Epoch 4/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.6031 - accuracy: 0.7104 - val_loss: 0.5916 - val_accuracy: 0.7222\n",
      "Epoch 5/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.5795 - accuracy: 0.7316 - val_loss: 0.5645 - val_accuracy: 0.7333\n",
      "Epoch 6/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.5598 - accuracy: 0.7391 - val_loss: 0.5461 - val_accuracy: 0.7444\n",
      "Epoch 7/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.5421 - accuracy: 0.7391 - val_loss: 0.5237 - val_accuracy: 0.7778\n",
      "Epoch 8/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.5268 - accuracy: 0.7541 - val_loss: 0.5055 - val_accuracy: 0.8222\n",
      "Epoch 9/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.5146 - accuracy: 0.7665 - val_loss: 0.4899 - val_accuracy: 0.8222\n",
      "Epoch 10/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.5039 - accuracy: 0.7690 - val_loss: 0.4767 - val_accuracy: 0.8333\n",
      "Epoch 11/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4944 - accuracy: 0.7665 - val_loss: 0.4655 - val_accuracy: 0.8111\n",
      "Epoch 12/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4875 - accuracy: 0.7690 - val_loss: 0.4560 - val_accuracy: 0.8222\n",
      "Epoch 13/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4831 - accuracy: 0.7653 - val_loss: 0.4520 - val_accuracy: 0.8333\n",
      "Epoch 14/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4784 - accuracy: 0.7728 - val_loss: 0.4441 - val_accuracy: 0.8333\n",
      "Epoch 15/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4732 - accuracy: 0.7778 - val_loss: 0.4394 - val_accuracy: 0.8444\n",
      "Epoch 16/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4693 - accuracy: 0.7815 - val_loss: 0.4348 - val_accuracy: 0.8333\n",
      "Epoch 17/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4665 - accuracy: 0.7828 - val_loss: 0.4323 - val_accuracy: 0.8222\n",
      "Epoch 18/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4636 - accuracy: 0.7853 - val_loss: 0.4281 - val_accuracy: 0.8222\n",
      "Epoch 19/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4605 - accuracy: 0.7865 - val_loss: 0.4233 - val_accuracy: 0.8111\n",
      "Epoch 20/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4578 - accuracy: 0.7878 - val_loss: 0.4199 - val_accuracy: 0.8111\n",
      "Epoch 21/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.4551 - accuracy: 0.7890 - val_loss: 0.4164 - val_accuracy: 0.8111\n",
      "Epoch 22/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4527 - accuracy: 0.7903 - val_loss: 0.4130 - val_accuracy: 0.8222\n",
      "Epoch 23/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4508 - accuracy: 0.7915 - val_loss: 0.4100 - val_accuracy: 0.8222\n",
      "Epoch 24/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4487 - accuracy: 0.7928 - val_loss: 0.4085 - val_accuracy: 0.8222\n",
      "Epoch 25/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4466 - accuracy: 0.7940 - val_loss: 0.4065 - val_accuracy: 0.8222\n",
      "Epoch 26/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4450 - accuracy: 0.7990 - val_loss: 0.4042 - val_accuracy: 0.8222\n",
      "Epoch 27/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4435 - accuracy: 0.7978 - val_loss: 0.4026 - val_accuracy: 0.8111\n",
      "Epoch 28/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4418 - accuracy: 0.8002 - val_loss: 0.4013 - val_accuracy: 0.8111\n",
      "Epoch 29/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4404 - accuracy: 0.8002 - val_loss: 0.4002 - val_accuracy: 0.8111\n",
      "Epoch 30/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4392 - accuracy: 0.8015 - val_loss: 0.3986 - val_accuracy: 0.8111\n",
      "Epoch 31/300\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4379 - accuracy: 0.8027 - val_loss: 0.3969 - val_accuracy: 0.8111\n",
      "Epoch 32/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4365 - accuracy: 0.8027 - val_loss: 0.3947 - val_accuracy: 0.8111\n",
      "Epoch 33/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4351 - accuracy: 0.8065 - val_loss: 0.3930 - val_accuracy: 0.8111\n",
      "Epoch 34/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4344 - accuracy: 0.8090 - val_loss: 0.3890 - val_accuracy: 0.8111\n",
      "Epoch 35/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4333 - accuracy: 0.8090 - val_loss: 0.3885 - val_accuracy: 0.8111\n",
      "Epoch 36/300\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4322 - accuracy: 0.8115 - val_loss: 0.3880 - val_accuracy: 0.8111\n",
      "Epoch 37/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.4310 - accuracy: 0.8115 - val_loss: 0.3867 - val_accuracy: 0.8111\n",
      "Epoch 38/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.4299 - accuracy: 0.8115 - val_loss: 0.3863 - val_accuracy: 0.8111\n",
      "Epoch 39/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.4289 - accuracy: 0.8127 - val_loss: 0.3865 - val_accuracy: 0.8111\n",
      "Epoch 40/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.4278 - accuracy: 0.8115 - val_loss: 0.3853 - val_accuracy: 0.8111\n",
      "Epoch 41/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4269 - accuracy: 0.8090 - val_loss: 0.3851 - val_accuracy: 0.8111\n",
      "Epoch 42/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4261 - accuracy: 0.8090 - val_loss: 0.3844 - val_accuracy: 0.8111\n",
      "Epoch 43/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4254 - accuracy: 0.8115 - val_loss: 0.3832 - val_accuracy: 0.8222\n",
      "Epoch 44/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4247 - accuracy: 0.8115 - val_loss: 0.3827 - val_accuracy: 0.8222\n",
      "Epoch 45/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4237 - accuracy: 0.8102 - val_loss: 0.3812 - val_accuracy: 0.8111\n",
      "Epoch 46/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4230 - accuracy: 0.8115 - val_loss: 0.3792 - val_accuracy: 0.8111\n",
      "Epoch 47/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.4222 - accuracy: 0.8140 - val_loss: 0.3782 - val_accuracy: 0.8111\n",
      "Epoch 48/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4214 - accuracy: 0.8127 - val_loss: 0.3771 - val_accuracy: 0.8111\n",
      "Epoch 49/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4207 - accuracy: 0.8127 - val_loss: 0.3772 - val_accuracy: 0.8222\n",
      "Epoch 50/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4199 - accuracy: 0.8127 - val_loss: 0.3765 - val_accuracy: 0.8222\n",
      "Epoch 51/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4193 - accuracy: 0.8152 - val_loss: 0.3758 - val_accuracy: 0.8222\n",
      "Epoch 52/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4182 - accuracy: 0.8152 - val_loss: 0.3756 - val_accuracy: 0.8222\n",
      "Epoch 53/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4175 - accuracy: 0.8165 - val_loss: 0.3752 - val_accuracy: 0.8222\n",
      "Epoch 54/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4186 - accuracy: 0.8227 - val_loss: 0.3788 - val_accuracy: 0.8222\n",
      "Epoch 55/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4174 - accuracy: 0.8202 - val_loss: 0.3773 - val_accuracy: 0.8222\n",
      "Epoch 56/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4158 - accuracy: 0.8227 - val_loss: 0.3762 - val_accuracy: 0.8222\n",
      "Epoch 57/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4149 - accuracy: 0.8227 - val_loss: 0.3746 - val_accuracy: 0.8222\n",
      "Epoch 58/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4146 - accuracy: 0.8227 - val_loss: 0.3747 - val_accuracy: 0.8222\n",
      "Epoch 59/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4145 - accuracy: 0.8215 - val_loss: 0.3724 - val_accuracy: 0.8222\n",
      "Epoch 60/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4137 - accuracy: 0.8215 - val_loss: 0.3716 - val_accuracy: 0.8222\n",
      "Epoch 61/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4129 - accuracy: 0.8227 - val_loss: 0.3696 - val_accuracy: 0.8222\n",
      "Epoch 62/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4128 - accuracy: 0.8252 - val_loss: 0.3652 - val_accuracy: 0.8222\n",
      "Epoch 63/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4121 - accuracy: 0.8215 - val_loss: 0.3662 - val_accuracy: 0.8222\n",
      "Epoch 64/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4117 - accuracy: 0.8215 - val_loss: 0.3658 - val_accuracy: 0.8222\n",
      "Epoch 65/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4112 - accuracy: 0.8252 - val_loss: 0.3680 - val_accuracy: 0.8111\n",
      "Epoch 66/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4106 - accuracy: 0.8252 - val_loss: 0.3675 - val_accuracy: 0.8222\n",
      "Epoch 67/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4099 - accuracy: 0.8252 - val_loss: 0.3700 - val_accuracy: 0.8222\n",
      "Epoch 68/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4094 - accuracy: 0.8227 - val_loss: 0.3684 - val_accuracy: 0.8222\n",
      "Epoch 69/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4091 - accuracy: 0.8240 - val_loss: 0.3674 - val_accuracy: 0.8222\n",
      "Epoch 70/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4086 - accuracy: 0.8240 - val_loss: 0.3665 - val_accuracy: 0.8222\n",
      "Epoch 71/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4082 - accuracy: 0.8240 - val_loss: 0.3653 - val_accuracy: 0.8222\n",
      "Epoch 72/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4080 - accuracy: 0.8240 - val_loss: 0.3647 - val_accuracy: 0.8222\n",
      "Epoch 73/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4078 - accuracy: 0.8227 - val_loss: 0.3641 - val_accuracy: 0.8222\n",
      "Epoch 74/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4073 - accuracy: 0.8252 - val_loss: 0.3638 - val_accuracy: 0.8111\n",
      "Epoch 75/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4066 - accuracy: 0.8265 - val_loss: 0.3630 - val_accuracy: 0.8111\n",
      "Epoch 76/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4064 - accuracy: 0.8277 - val_loss: 0.3646 - val_accuracy: 0.8111\n",
      "Epoch 77/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4068 - accuracy: 0.8252 - val_loss: 0.3591 - val_accuracy: 0.8222\n",
      "Epoch 78/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4067 - accuracy: 0.8215 - val_loss: 0.3590 - val_accuracy: 0.8222\n",
      "Epoch 79/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4060 - accuracy: 0.8265 - val_loss: 0.3550 - val_accuracy: 0.8222\n",
      "Epoch 80/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4058 - accuracy: 0.8277 - val_loss: 0.3551 - val_accuracy: 0.8222\n",
      "Epoch 81/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4052 - accuracy: 0.8277 - val_loss: 0.3550 - val_accuracy: 0.8222\n",
      "Epoch 82/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4048 - accuracy: 0.8290 - val_loss: 0.3558 - val_accuracy: 0.8222\n",
      "Epoch 83/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4042 - accuracy: 0.8302 - val_loss: 0.3556 - val_accuracy: 0.8222\n",
      "Epoch 84/300\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4039 - accuracy: 0.8277 - val_loss: 0.3564 - val_accuracy: 0.8222\n",
      "Epoch 85/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.4033 - accuracy: 0.8277 - val_loss: 0.3560 - val_accuracy: 0.8222\n",
      "Epoch 86/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.4032 - accuracy: 0.8252 - val_loss: 0.3558 - val_accuracy: 0.8333\n",
      "Epoch 87/300\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4026 - accuracy: 0.8290 - val_loss: 0.3551 - val_accuracy: 0.8333\n",
      "Epoch 88/300\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4023 - accuracy: 0.8302 - val_loss: 0.3568 - val_accuracy: 0.8222\n",
      "Epoch 89/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.4025 - accuracy: 0.8302 - val_loss: 0.3560 - val_accuracy: 0.8222\n",
      "Epoch 90/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.4018 - accuracy: 0.8290 - val_loss: 0.3553 - val_accuracy: 0.8222\n",
      "Epoch 91/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.4014 - accuracy: 0.8290 - val_loss: 0.3550 - val_accuracy: 0.8222\n",
      "Epoch 92/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.4012 - accuracy: 0.8290 - val_loss: 0.3551 - val_accuracy: 0.8222\n",
      "Epoch 93/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.4010 - accuracy: 0.8315 - val_loss: 0.3580 - val_accuracy: 0.8222\n",
      "Epoch 94/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.4019 - accuracy: 0.8290 - val_loss: 0.3589 - val_accuracy: 0.8333\n",
      "Epoch 95/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.4007 - accuracy: 0.8315 - val_loss: 0.3574 - val_accuracy: 0.8222\n",
      "Epoch 96/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3999 - accuracy: 0.8302 - val_loss: 0.3550 - val_accuracy: 0.8222\n",
      "Epoch 97/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3991 - accuracy: 0.8302 - val_loss: 0.3549 - val_accuracy: 0.8222\n",
      "Epoch 98/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3988 - accuracy: 0.8290 - val_loss: 0.3543 - val_accuracy: 0.8222\n",
      "Epoch 99/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3990 - accuracy: 0.8265 - val_loss: 0.3526 - val_accuracy: 0.8222\n",
      "Epoch 100/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3984 - accuracy: 0.8277 - val_loss: 0.3529 - val_accuracy: 0.8222\n",
      "Epoch 101/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3979 - accuracy: 0.8265 - val_loss: 0.3521 - val_accuracy: 0.8222\n",
      "Epoch 102/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3974 - accuracy: 0.8315 - val_loss: 0.3512 - val_accuracy: 0.8222\n",
      "Epoch 103/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3972 - accuracy: 0.8340 - val_loss: 0.3513 - val_accuracy: 0.8222\n",
      "Epoch 104/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3961 - accuracy: 0.8315 - val_loss: 0.3514 - val_accuracy: 0.8222\n",
      "Epoch 105/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3972 - accuracy: 0.8265 - val_loss: 0.3506 - val_accuracy: 0.8222\n",
      "Epoch 106/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3965 - accuracy: 0.8315 - val_loss: 0.3498 - val_accuracy: 0.8222\n",
      "Epoch 107/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3959 - accuracy: 0.8290 - val_loss: 0.3493 - val_accuracy: 0.8222\n",
      "Epoch 108/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3957 - accuracy: 0.8315 - val_loss: 0.3494 - val_accuracy: 0.8222\n",
      "Epoch 109/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3956 - accuracy: 0.8327 - val_loss: 0.3487 - val_accuracy: 0.8222\n",
      "Epoch 110/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3952 - accuracy: 0.8315 - val_loss: 0.3483 - val_accuracy: 0.8222\n",
      "Epoch 111/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3947 - accuracy: 0.8327 - val_loss: 0.3488 - val_accuracy: 0.8222\n",
      "Epoch 112/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3952 - accuracy: 0.8302 - val_loss: 0.3491 - val_accuracy: 0.8222\n",
      "Epoch 113/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3941 - accuracy: 0.8315 - val_loss: 0.3487 - val_accuracy: 0.8222\n",
      "Epoch 114/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3940 - accuracy: 0.8302 - val_loss: 0.3487 - val_accuracy: 0.8222\n",
      "Epoch 115/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3936 - accuracy: 0.8327 - val_loss: 0.3492 - val_accuracy: 0.8222\n",
      "Epoch 116/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3952 - accuracy: 0.8315 - val_loss: 0.3513 - val_accuracy: 0.8222\n",
      "Epoch 117/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3949 - accuracy: 0.8290 - val_loss: 0.3478 - val_accuracy: 0.8222\n",
      "Epoch 118/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3944 - accuracy: 0.8302 - val_loss: 0.3485 - val_accuracy: 0.8222\n",
      "Epoch 119/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3936 - accuracy: 0.8327 - val_loss: 0.3493 - val_accuracy: 0.8222\n",
      "Epoch 120/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.3930 - accuracy: 0.8352 - val_loss: 0.3493 - val_accuracy: 0.8222\n",
      "Epoch 121/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3929 - accuracy: 0.8302 - val_loss: 0.3482 - val_accuracy: 0.8222\n",
      "Epoch 122/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3922 - accuracy: 0.8302 - val_loss: 0.3492 - val_accuracy: 0.8222\n",
      "Epoch 123/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3921 - accuracy: 0.8327 - val_loss: 0.3498 - val_accuracy: 0.8222\n",
      "Epoch 124/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3918 - accuracy: 0.8290 - val_loss: 0.3504 - val_accuracy: 0.8222\n",
      "Epoch 125/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3917 - accuracy: 0.8315 - val_loss: 0.3499 - val_accuracy: 0.8222\n",
      "Epoch 126/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3914 - accuracy: 0.8327 - val_loss: 0.3492 - val_accuracy: 0.8222\n",
      "Epoch 127/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3914 - accuracy: 0.8365 - val_loss: 0.3515 - val_accuracy: 0.8333\n",
      "Epoch 128/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3911 - accuracy: 0.8340 - val_loss: 0.3513 - val_accuracy: 0.8333\n",
      "Epoch 129/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3905 - accuracy: 0.8352 - val_loss: 0.3507 - val_accuracy: 0.8222\n",
      "Epoch 130/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3905 - accuracy: 0.8352 - val_loss: 0.3517 - val_accuracy: 0.8222\n",
      "Epoch 131/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3902 - accuracy: 0.8315 - val_loss: 0.3526 - val_accuracy: 0.8222\n",
      "Epoch 132/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3899 - accuracy: 0.8352 - val_loss: 0.3511 - val_accuracy: 0.8222\n",
      "Epoch 133/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3894 - accuracy: 0.8365 - val_loss: 0.3502 - val_accuracy: 0.8222\n",
      "Epoch 134/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3894 - accuracy: 0.8377 - val_loss: 0.3498 - val_accuracy: 0.8222\n",
      "Epoch 135/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.3897 - accuracy: 0.8340 - val_loss: 0.3544 - val_accuracy: 0.8333\n",
      "Epoch 136/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3904 - accuracy: 0.8352 - val_loss: 0.3526 - val_accuracy: 0.8333\n",
      "Epoch 137/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.3897 - accuracy: 0.8340 - val_loss: 0.3520 - val_accuracy: 0.8333\n",
      "Epoch 138/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3893 - accuracy: 0.8340 - val_loss: 0.3536 - val_accuracy: 0.8222\n",
      "Epoch 139/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3903 - accuracy: 0.8340 - val_loss: 0.3483 - val_accuracy: 0.8222\n",
      "Epoch 140/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3890 - accuracy: 0.8340 - val_loss: 0.3479 - val_accuracy: 0.8222\n",
      "Epoch 141/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3883 - accuracy: 0.8327 - val_loss: 0.3476 - val_accuracy: 0.8222\n",
      "Epoch 142/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3877 - accuracy: 0.8340 - val_loss: 0.3468 - val_accuracy: 0.8222\n",
      "Epoch 143/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3875 - accuracy: 0.8340 - val_loss: 0.3467 - val_accuracy: 0.8222\n",
      "Epoch 144/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3870 - accuracy: 0.8365 - val_loss: 0.3474 - val_accuracy: 0.8222\n",
      "Epoch 145/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3869 - accuracy: 0.8340 - val_loss: 0.3475 - val_accuracy: 0.8222\n",
      "Epoch 146/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3873 - accuracy: 0.8315 - val_loss: 0.3473 - val_accuracy: 0.8222\n",
      "Epoch 147/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3885 - accuracy: 0.8252 - val_loss: 0.3430 - val_accuracy: 0.8333\n",
      "Epoch 148/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3881 - accuracy: 0.8240 - val_loss: 0.3419 - val_accuracy: 0.8222\n",
      "Epoch 149/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3867 - accuracy: 0.8340 - val_loss: 0.3437 - val_accuracy: 0.8222\n",
      "Epoch 150/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3864 - accuracy: 0.8315 - val_loss: 0.3435 - val_accuracy: 0.8222\n",
      "Epoch 151/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3858 - accuracy: 0.8327 - val_loss: 0.3441 - val_accuracy: 0.8222\n",
      "Epoch 152/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3855 - accuracy: 0.8340 - val_loss: 0.3444 - val_accuracy: 0.8222\n",
      "Epoch 153/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3852 - accuracy: 0.8365 - val_loss: 0.3441 - val_accuracy: 0.8222\n",
      "Epoch 154/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3851 - accuracy: 0.8365 - val_loss: 0.3441 - val_accuracy: 0.8222\n",
      "Epoch 155/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3867 - accuracy: 0.8315 - val_loss: 0.3446 - val_accuracy: 0.8222\n",
      "Epoch 156/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3851 - accuracy: 0.8352 - val_loss: 0.3463 - val_accuracy: 0.8222\n",
      "Epoch 157/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3847 - accuracy: 0.8315 - val_loss: 0.3472 - val_accuracy: 0.8222\n",
      "Epoch 158/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3843 - accuracy: 0.8365 - val_loss: 0.3475 - val_accuracy: 0.8222\n",
      "Epoch 159/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3842 - accuracy: 0.8377 - val_loss: 0.3461 - val_accuracy: 0.8222\n",
      "Epoch 160/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.3838 - accuracy: 0.8377 - val_loss: 0.3462 - val_accuracy: 0.8222\n",
      "Epoch 161/300\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3836 - accuracy: 0.8390 - val_loss: 0.3463 - val_accuracy: 0.8222\n",
      "Epoch 162/300\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3838 - accuracy: 0.8402 - val_loss: 0.3476 - val_accuracy: 0.8222\n",
      "Epoch 163/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.3838 - accuracy: 0.8390 - val_loss: 0.3508 - val_accuracy: 0.8333\n",
      "Epoch 164/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.3837 - accuracy: 0.8402 - val_loss: 0.3501 - val_accuracy: 0.8222\n",
      "Epoch 165/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.3832 - accuracy: 0.8414 - val_loss: 0.3486 - val_accuracy: 0.8222\n",
      "Epoch 166/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3828 - accuracy: 0.8402 - val_loss: 0.3480 - val_accuracy: 0.8222\n",
      "Epoch 167/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3823 - accuracy: 0.8402 - val_loss: 0.3479 - val_accuracy: 0.8222\n",
      "Epoch 168/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3829 - accuracy: 0.8390 - val_loss: 0.3498 - val_accuracy: 0.8222\n",
      "Epoch 169/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3825 - accuracy: 0.8402 - val_loss: 0.3491 - val_accuracy: 0.8222\n",
      "Epoch 170/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3824 - accuracy: 0.8414 - val_loss: 0.3522 - val_accuracy: 0.8333\n",
      "Epoch 171/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3821 - accuracy: 0.8377 - val_loss: 0.3512 - val_accuracy: 0.8333\n",
      "Epoch 172/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3816 - accuracy: 0.8390 - val_loss: 0.3488 - val_accuracy: 0.8333\n",
      "Epoch 173/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3814 - accuracy: 0.8402 - val_loss: 0.3482 - val_accuracy: 0.8333\n",
      "Epoch 174/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3815 - accuracy: 0.8377 - val_loss: 0.3486 - val_accuracy: 0.8333\n",
      "Epoch 175/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3816 - accuracy: 0.8390 - val_loss: 0.3513 - val_accuracy: 0.8333\n",
      "Epoch 176/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3819 - accuracy: 0.8390 - val_loss: 0.3446 - val_accuracy: 0.8333\n",
      "Epoch 177/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3816 - accuracy: 0.8402 - val_loss: 0.3429 - val_accuracy: 0.8333\n",
      "Epoch 178/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3813 - accuracy: 0.8390 - val_loss: 0.3425 - val_accuracy: 0.8333\n",
      "Epoch 179/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3814 - accuracy: 0.8390 - val_loss: 0.3434 - val_accuracy: 0.8333\n",
      "Epoch 180/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3810 - accuracy: 0.8390 - val_loss: 0.3446 - val_accuracy: 0.8333\n",
      "Epoch 181/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3804 - accuracy: 0.8402 - val_loss: 0.3456 - val_accuracy: 0.8222\n",
      "Epoch 182/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3801 - accuracy: 0.8414 - val_loss: 0.3478 - val_accuracy: 0.8222\n",
      "Epoch 183/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3801 - accuracy: 0.8402 - val_loss: 0.3473 - val_accuracy: 0.8222\n",
      "Epoch 184/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3810 - accuracy: 0.8352 - val_loss: 0.3453 - val_accuracy: 0.8222\n",
      "Epoch 185/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3800 - accuracy: 0.8365 - val_loss: 0.3472 - val_accuracy: 0.8333\n",
      "Epoch 186/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3800 - accuracy: 0.8377 - val_loss: 0.3502 - val_accuracy: 0.8333\n",
      "Epoch 187/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3798 - accuracy: 0.8377 - val_loss: 0.3490 - val_accuracy: 0.8333\n",
      "Epoch 188/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3796 - accuracy: 0.8452 - val_loss: 0.3515 - val_accuracy: 0.8333\n",
      "Epoch 189/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3791 - accuracy: 0.8452 - val_loss: 0.3481 - val_accuracy: 0.8333\n",
      "Epoch 190/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3787 - accuracy: 0.8427 - val_loss: 0.3475 - val_accuracy: 0.8333\n",
      "Epoch 191/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3782 - accuracy: 0.8414 - val_loss: 0.3473 - val_accuracy: 0.8333\n",
      "Epoch 192/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.3781 - accuracy: 0.8414 - val_loss: 0.3464 - val_accuracy: 0.8333\n",
      "Epoch 193/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3782 - accuracy: 0.8414 - val_loss: 0.3456 - val_accuracy: 0.8333\n",
      "Epoch 194/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3782 - accuracy: 0.8414 - val_loss: 0.3449 - val_accuracy: 0.8333\n",
      "Epoch 195/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3778 - accuracy: 0.8414 - val_loss: 0.3452 - val_accuracy: 0.8333\n",
      "Epoch 196/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3779 - accuracy: 0.8414 - val_loss: 0.3492 - val_accuracy: 0.8333\n",
      "Epoch 197/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3778 - accuracy: 0.8414 - val_loss: 0.3483 - val_accuracy: 0.8333\n",
      "Epoch 198/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3776 - accuracy: 0.8414 - val_loss: 0.3481 - val_accuracy: 0.8333\n",
      "Epoch 199/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3773 - accuracy: 0.8414 - val_loss: 0.3465 - val_accuracy: 0.8333\n",
      "Epoch 200/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3769 - accuracy: 0.8427 - val_loss: 0.3471 - val_accuracy: 0.8333\n",
      "Epoch 201/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3769 - accuracy: 0.8427 - val_loss: 0.3464 - val_accuracy: 0.8333\n",
      "Epoch 202/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3769 - accuracy: 0.8439 - val_loss: 0.3436 - val_accuracy: 0.8333\n",
      "Epoch 203/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3772 - accuracy: 0.8402 - val_loss: 0.3449 - val_accuracy: 0.8333\n",
      "Epoch 204/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.3769 - accuracy: 0.8414 - val_loss: 0.3421 - val_accuracy: 0.8444\n",
      "Epoch 205/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3764 - accuracy: 0.8414 - val_loss: 0.3420 - val_accuracy: 0.8444\n",
      "Epoch 206/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3763 - accuracy: 0.8414 - val_loss: 0.3419 - val_accuracy: 0.8333\n",
      "Epoch 207/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3759 - accuracy: 0.8427 - val_loss: 0.3437 - val_accuracy: 0.8333\n",
      "Epoch 208/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3756 - accuracy: 0.8414 - val_loss: 0.3443 - val_accuracy: 0.8333\n",
      "Epoch 209/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3766 - accuracy: 0.8427 - val_loss: 0.3425 - val_accuracy: 0.8222\n",
      "Epoch 210/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3756 - accuracy: 0.8402 - val_loss: 0.3417 - val_accuracy: 0.8222\n",
      "Epoch 211/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3753 - accuracy: 0.8427 - val_loss: 0.3412 - val_accuracy: 0.8222\n",
      "Epoch 212/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3750 - accuracy: 0.8414 - val_loss: 0.3427 - val_accuracy: 0.8222\n",
      "Epoch 213/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3749 - accuracy: 0.8402 - val_loss: 0.3486 - val_accuracy: 0.8333\n",
      "Epoch 214/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3752 - accuracy: 0.8414 - val_loss: 0.3483 - val_accuracy: 0.8333\n",
      "Epoch 215/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3748 - accuracy: 0.8390 - val_loss: 0.3481 - val_accuracy: 0.8333\n",
      "Epoch 216/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3744 - accuracy: 0.8402 - val_loss: 0.3468 - val_accuracy: 0.8333\n",
      "Epoch 217/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3742 - accuracy: 0.8439 - val_loss: 0.3467 - val_accuracy: 0.8333\n",
      "Epoch 218/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3742 - accuracy: 0.8414 - val_loss: 0.3459 - val_accuracy: 0.8333\n",
      "Epoch 219/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3745 - accuracy: 0.8377 - val_loss: 0.3467 - val_accuracy: 0.8333\n",
      "Epoch 220/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3744 - accuracy: 0.8402 - val_loss: 0.3465 - val_accuracy: 0.8222\n",
      "Epoch 221/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3741 - accuracy: 0.8414 - val_loss: 0.3462 - val_accuracy: 0.8333\n",
      "Epoch 222/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3738 - accuracy: 0.8414 - val_loss: 0.3460 - val_accuracy: 0.8333\n",
      "Epoch 223/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3735 - accuracy: 0.8439 - val_loss: 0.3495 - val_accuracy: 0.8333\n",
      "Epoch 224/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3759 - accuracy: 0.8464 - val_loss: 0.3526 - val_accuracy: 0.8333\n",
      "Epoch 225/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3753 - accuracy: 0.8439 - val_loss: 0.3500 - val_accuracy: 0.8333\n",
      "Epoch 226/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3742 - accuracy: 0.8439 - val_loss: 0.3493 - val_accuracy: 0.8333\n",
      "Epoch 227/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3740 - accuracy: 0.8439 - val_loss: 0.3482 - val_accuracy: 0.8333\n",
      "Epoch 228/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3736 - accuracy: 0.8439 - val_loss: 0.3478 - val_accuracy: 0.8333\n",
      "Epoch 229/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3733 - accuracy: 0.8439 - val_loss: 0.3474 - val_accuracy: 0.8333\n",
      "Epoch 230/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3730 - accuracy: 0.8427 - val_loss: 0.3474 - val_accuracy: 0.8222\n",
      "Epoch 231/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3730 - accuracy: 0.8439 - val_loss: 0.3472 - val_accuracy: 0.8222\n",
      "Epoch 232/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3729 - accuracy: 0.8452 - val_loss: 0.3475 - val_accuracy: 0.8333\n",
      "Epoch 233/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3725 - accuracy: 0.8464 - val_loss: 0.3476 - val_accuracy: 0.8222\n",
      "Epoch 234/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3724 - accuracy: 0.8452 - val_loss: 0.3474 - val_accuracy: 0.8333\n",
      "Epoch 235/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3725 - accuracy: 0.8427 - val_loss: 0.3468 - val_accuracy: 0.8222\n",
      "Epoch 236/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3723 - accuracy: 0.8452 - val_loss: 0.3463 - val_accuracy: 0.8222\n",
      "Epoch 237/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3720 - accuracy: 0.8439 - val_loss: 0.3460 - val_accuracy: 0.8222\n",
      "Epoch 238/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3721 - accuracy: 0.8452 - val_loss: 0.3465 - val_accuracy: 0.8222\n",
      "Epoch 239/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3735 - accuracy: 0.8452 - val_loss: 0.3473 - val_accuracy: 0.8333\n",
      "Epoch 240/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.3725 - accuracy: 0.8427 - val_loss: 0.3470 - val_accuracy: 0.8333\n",
      "Epoch 241/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3721 - accuracy: 0.8439 - val_loss: 0.3484 - val_accuracy: 0.8222\n",
      "Epoch 242/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3718 - accuracy: 0.8390 - val_loss: 0.3474 - val_accuracy: 0.8222\n",
      "Epoch 243/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3714 - accuracy: 0.8452 - val_loss: 0.3472 - val_accuracy: 0.8222\n",
      "Epoch 244/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3714 - accuracy: 0.8427 - val_loss: 0.3473 - val_accuracy: 0.8333\n",
      "Epoch 245/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3716 - accuracy: 0.8439 - val_loss: 0.3467 - val_accuracy: 0.8333\n",
      "Epoch 246/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3712 - accuracy: 0.8427 - val_loss: 0.3459 - val_accuracy: 0.8333\n",
      "Epoch 247/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3707 - accuracy: 0.8464 - val_loss: 0.3470 - val_accuracy: 0.8333\n",
      "Epoch 248/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3713 - accuracy: 0.8439 - val_loss: 0.3513 - val_accuracy: 0.8333\n",
      "Epoch 249/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3713 - accuracy: 0.8464 - val_loss: 0.3501 - val_accuracy: 0.8333\n",
      "Epoch 250/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.3713 - accuracy: 0.8452 - val_loss: 0.3480 - val_accuracy: 0.8333\n",
      "Epoch 251/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3713 - accuracy: 0.8452 - val_loss: 0.3484 - val_accuracy: 0.8333\n",
      "Epoch 252/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3712 - accuracy: 0.8452 - val_loss: 0.3466 - val_accuracy: 0.8333\n",
      "Epoch 253/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.3709 - accuracy: 0.8477 - val_loss: 0.3462 - val_accuracy: 0.8333\n",
      "Epoch 254/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3714 - accuracy: 0.8489 - val_loss: 0.3503 - val_accuracy: 0.8333\n",
      "Epoch 255/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3706 - accuracy: 0.8452 - val_loss: 0.3413 - val_accuracy: 0.8444\n",
      "Epoch 256/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3707 - accuracy: 0.8477 - val_loss: 0.3414 - val_accuracy: 0.8444\n",
      "Epoch 257/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3706 - accuracy: 0.8477 - val_loss: 0.3415 - val_accuracy: 0.8444\n",
      "Epoch 258/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3703 - accuracy: 0.8477 - val_loss: 0.3422 - val_accuracy: 0.8333\n",
      "Epoch 259/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3715 - accuracy: 0.8452 - val_loss: 0.3402 - val_accuracy: 0.8556\n",
      "Epoch 260/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.3703 - accuracy: 0.8439 - val_loss: 0.3416 - val_accuracy: 0.8444\n",
      "Epoch 261/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3701 - accuracy: 0.8464 - val_loss: 0.3434 - val_accuracy: 0.8444\n",
      "Epoch 262/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3700 - accuracy: 0.8452 - val_loss: 0.3458 - val_accuracy: 0.8444\n",
      "Epoch 263/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3700 - accuracy: 0.8464 - val_loss: 0.3465 - val_accuracy: 0.8444\n",
      "Epoch 264/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3700 - accuracy: 0.8477 - val_loss: 0.3462 - val_accuracy: 0.8333\n",
      "Epoch 265/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3702 - accuracy: 0.8502 - val_loss: 0.3475 - val_accuracy: 0.8222\n",
      "Epoch 266/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3692 - accuracy: 0.8452 - val_loss: 0.3518 - val_accuracy: 0.8333\n",
      "Epoch 267/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3696 - accuracy: 0.8477 - val_loss: 0.3528 - val_accuracy: 0.8333\n",
      "Epoch 268/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3702 - accuracy: 0.8464 - val_loss: 0.3500 - val_accuracy: 0.8333\n",
      "Epoch 269/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3696 - accuracy: 0.8464 - val_loss: 0.3496 - val_accuracy: 0.8333\n",
      "Epoch 270/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3694 - accuracy: 0.8489 - val_loss: 0.3479 - val_accuracy: 0.8333\n",
      "Epoch 271/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3689 - accuracy: 0.8489 - val_loss: 0.3465 - val_accuracy: 0.8333\n",
      "Epoch 272/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3692 - accuracy: 0.8489 - val_loss: 0.3465 - val_accuracy: 0.8333\n",
      "Epoch 273/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3701 - accuracy: 0.8514 - val_loss: 0.3466 - val_accuracy: 0.8444\n",
      "Epoch 274/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3698 - accuracy: 0.8514 - val_loss: 0.3432 - val_accuracy: 0.8333\n",
      "Epoch 275/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3693 - accuracy: 0.8527 - val_loss: 0.3435 - val_accuracy: 0.8333\n",
      "Epoch 276/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3690 - accuracy: 0.8514 - val_loss: 0.3438 - val_accuracy: 0.8333\n",
      "Epoch 277/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3688 - accuracy: 0.8514 - val_loss: 0.3441 - val_accuracy: 0.8333\n",
      "Epoch 278/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3686 - accuracy: 0.8502 - val_loss: 0.3442 - val_accuracy: 0.8333\n",
      "Epoch 279/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3685 - accuracy: 0.8502 - val_loss: 0.3438 - val_accuracy: 0.8333\n",
      "Epoch 280/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3693 - accuracy: 0.8514 - val_loss: 0.3428 - val_accuracy: 0.8333\n",
      "Epoch 281/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3693 - accuracy: 0.8477 - val_loss: 0.3432 - val_accuracy: 0.8333\n",
      "Epoch 282/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3689 - accuracy: 0.8464 - val_loss: 0.3431 - val_accuracy: 0.8222\n",
      "Epoch 283/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3689 - accuracy: 0.8477 - val_loss: 0.3463 - val_accuracy: 0.8333\n",
      "Epoch 284/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3681 - accuracy: 0.8477 - val_loss: 0.3475 - val_accuracy: 0.8333\n",
      "Epoch 285/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3682 - accuracy: 0.8489 - val_loss: 0.3467 - val_accuracy: 0.8333\n",
      "Epoch 286/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3681 - accuracy: 0.8489 - val_loss: 0.3463 - val_accuracy: 0.8333\n",
      "Epoch 287/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3683 - accuracy: 0.8502 - val_loss: 0.3460 - val_accuracy: 0.8333\n",
      "Epoch 288/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3681 - accuracy: 0.8502 - val_loss: 0.3468 - val_accuracy: 0.8333\n",
      "Epoch 289/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3678 - accuracy: 0.8514 - val_loss: 0.3460 - val_accuracy: 0.8333\n",
      "Epoch 290/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3678 - accuracy: 0.8477 - val_loss: 0.3465 - val_accuracy: 0.8333\n",
      "Epoch 291/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3676 - accuracy: 0.8489 - val_loss: 0.3460 - val_accuracy: 0.8333\n",
      "Epoch 292/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3681 - accuracy: 0.8489 - val_loss: 0.3473 - val_accuracy: 0.8444\n",
      "Epoch 293/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3677 - accuracy: 0.8477 - val_loss: 0.3477 - val_accuracy: 0.8333\n",
      "Epoch 294/300\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3676 - accuracy: 0.8477 - val_loss: 0.3482 - val_accuracy: 0.8333\n",
      "Epoch 295/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.3673 - accuracy: 0.8489 - val_loss: 0.3486 - val_accuracy: 0.8333\n",
      "Epoch 296/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.3672 - accuracy: 0.8489 - val_loss: 0.3512 - val_accuracy: 0.8333\n",
      "Epoch 297/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.3677 - accuracy: 0.8502 - val_loss: 0.3551 - val_accuracy: 0.8333\n",
      "Epoch 298/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.3684 - accuracy: 0.8489 - val_loss: 0.3562 - val_accuracy: 0.8333\n",
      "Epoch 299/300\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.3676 - accuracy: 0.8489 - val_loss: 0.3529 - val_accuracy: 0.8333\n",
      "Epoch 300/300\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.3671 - accuracy: 0.8489 - val_loss: 0.3505 - val_accuracy: 0.8333\n"
     ]
    }
   ],
   "source": [
    "fit = model.fit(\n",
    "    X,\n",
    "    Y,\n",
    "    shuffle=True,\n",
    "    epochs=300,\n",
    "    validation_split=0.1,\n",
    "    verbose=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "397214e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T08:48:53.325035Z",
     "iopub.status.busy": "2022-09-27T08:48:53.324179Z",
     "iopub.status.idle": "2022-09-27T08:48:54.193275Z",
     "shell.execute_reply": "2022-09-27T08:48:54.192336Z"
    },
    "papermill": {
     "duration": 0.923168,
     "end_time": "2022-09-27T08:48:54.195329",
     "exception": false,
     "start_time": "2022-09-27T08:48:53.272161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.14.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"5e07f8d5-5307-46fb-a7e1-7f2f784e257a\" class=\"plotly-graph-div\" style=\"height:768px; width:1024px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5e07f8d5-5307-46fb-a7e1-7f2f784e257a\")) {                    Plotly.newPlot(                        \"5e07f8d5-5307-46fb-a7e1-7f2f784e257a\",                        [{\"line\":{\"color\":\"#e69f00\"},\"mode\":\"lines\",\"name\":\"train loss\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299],\"y\":[0.6996796131134033,0.6565809845924377,0.6275216341018677,0.6031447052955627,0.579545795917511,0.5598188638687134,0.5420882105827332,0.5268095135688782,0.5146361589431763,0.5039315223693848,0.4944055676460266,0.4874744713306427,0.48309969902038574,0.47841721773147583,0.4732127785682678,0.46934354305267334,0.4664956331253052,0.46363386511802673,0.4605425000190735,0.45780086517333984,0.4551309645175934,0.4527137577533722,0.4508175849914551,0.44868478178977966,0.44663456082344055,0.44497013092041016,0.44353073835372925,0.4417779743671417,0.4403722286224365,0.4391840994358063,0.43786633014678955,0.436527818441391,0.4350627064704895,0.434352308511734,0.4333317279815674,0.43220654129981995,0.431020051240921,0.42991599440574646,0.42894887924194336,0.4278482496738434,0.42686983942985535,0.4261241853237152,0.42541444301605225,0.42466965317726135,0.4236679971218109,0.42296648025512695,0.42216798663139343,0.421420156955719,0.4207215905189514,0.41988345980644226,0.4192730486392975,0.41824477910995483,0.41748130321502686,0.41856279969215393,0.4173865020275116,0.41577309370040894,0.41492292284965515,0.4146374464035034,0.4145011007785797,0.41374441981315613,0.41288089752197266,0.4127688407897949,0.4121374189853668,0.4117271900177002,0.41122519969940186,0.41057687997817993,0.40994226932525635,0.40944600105285645,0.4090676009654999,0.4085741639137268,0.4081972539424896,0.40802106261253357,0.40784892439842224,0.4072703719139099,0.40661734342575073,0.40641677379608154,0.40680626034736633,0.4066924750804901,0.4059523940086365,0.4058286249637604,0.40519818663597107,0.4047669470310211,0.40416058897972107,0.40386515855789185,0.4032595753669739,0.40319299697875977,0.40264204144477844,0.4023023843765259,0.4024568796157837,0.40177613496780396,0.40143051743507385,0.40117189288139343,0.40097445249557495,0.40189826488494873,0.400714248418808,0.3998953700065613,0.39914050698280334,0.3987642824649811,0.39903390407562256,0.39837124943733215,0.39793676137924194,0.3973968029022217,0.39723166823387146,0.3961031138896942,0.3972247540950775,0.3965286612510681,0.39590224623680115,0.39568546414375305,0.3956264853477478,0.3951513469219208,0.3946879804134369,0.39521417021751404,0.3941429853439331,0.3940317630767822,0.3935605585575104,0.3952327072620392,0.394898921251297,0.3943820595741272,0.39359983801841736,0.3929843008518219,0.39285629987716675,0.3921702206134796,0.39210665225982666,0.3917759358882904,0.39174479246139526,0.3914228081703186,0.39141547679901123,0.39108455181121826,0.39053672552108765,0.390529602766037,0.39023274183273315,0.3899163007736206,0.38939979672431946,0.3894130289554596,0.3896639347076416,0.39042311906814575,0.38972529768943787,0.38931506872177124,0.3902641236782074,0.3889654278755188,0.38832661509513855,0.3876878023147583,0.3875011205673218,0.38699913024902344,0.38694044947624207,0.3872501850128174,0.38846203684806824,0.3881175220012665,0.3867477476596832,0.38639795780181885,0.3858226537704468,0.38551655411720276,0.38523852825164795,0.3850899636745453,0.3866831958293915,0.38513439893722534,0.3847399950027466,0.38430389761924744,0.3841782808303833,0.38380998373031616,0.38364678621292114,0.3837796449661255,0.3838004469871521,0.38373368978500366,0.38319113850593567,0.3828187584877014,0.38228943943977356,0.38287943601608276,0.382526159286499,0.38237473368644714,0.38213059306144714,0.3815833628177643,0.38141483068466187,0.3814900815486908,0.38164082169532776,0.3819164037704468,0.38161501288414,0.38129952549934387,0.38143712282180786,0.3809964060783386,0.3803955614566803,0.3801317811012268,0.3801077902317047,0.38097089529037476,0.3800329566001892,0.37999334931373596,0.37979656457901,0.3796062469482422,0.37907153367996216,0.3786793649196625,0.3781881034374237,0.37810853123664856,0.37817591428756714,0.3781890273094177,0.3777619004249573,0.3778643012046814,0.37782928347587585,0.3775932192802429,0.3772517740726471,0.3769376873970032,0.3768619894981384,0.37693628668785095,0.3772360384464264,0.37685611844062805,0.3764444589614868,0.37628501653671265,0.37585118412971497,0.37561431527137756,0.37655848264694214,0.3755718469619751,0.37534573674201965,0.37503814697265625,0.37486550211906433,0.3751910924911499,0.3748050034046173,0.3744104206562042,0.3741556704044342,0.37418487668037415,0.374485582113266,0.37439897656440735,0.37409359216690063,0.3738415837287903,0.3734966814517975,0.37588196992874146,0.37529054284095764,0.37424126267433167,0.37402987480163574,0.3735966980457306,0.3732823431491852,0.3730296790599823,0.3730091154575348,0.3729412257671356,0.3725183606147766,0.3724415898323059,0.37252771854400635,0.37228912115097046,0.37201690673828125,0.37208059430122375,0.37348249554634094,0.3724769949913025,0.37208423018455505,0.37178534269332886,0.3714127540588379,0.3713611960411072,0.37161991000175476,0.37121134996414185,0.37070000171661377,0.37131768465042114,0.3712954521179199,0.3712671101093292,0.3712705075740814,0.37120765447616577,0.3708515465259552,0.3713681697845459,0.370635449886322,0.37066030502319336,0.37059465050697327,0.3703249394893646,0.37148556113243103,0.37028512358665466,0.37010136246681213,0.3700070381164551,0.3700072467327118,0.3699563443660736,0.3702315092086792,0.36917492747306824,0.3695682883262634,0.3701883554458618,0.3695603311061859,0.36937999725341797,0.3689485192298889,0.36922651529312134,0.37007877230644226,0.36983567476272583,0.3692736029624939,0.36900943517684937,0.3687828481197357,0.3685699999332428,0.3684660792350769,0.3692966401576996,0.3693447709083557,0.3689468801021576,0.3688690662384033,0.36814165115356445,0.3681655526161194,0.36807554960250854,0.3683456480503082,0.3681371212005615,0.3678198456764221,0.3678343594074249,0.36764028668403625,0.36813509464263916,0.36771243810653687,0.3675616681575775,0.3673410415649414,0.36717817187309265,0.36767974495887756,0.36835432052612305,0.36757341027259827,0.367109477519989],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"line\":{\"color\":\"#56b4e9\"},\"mode\":\"lines\",\"name\":\"validation loss\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299],\"y\":[0.6878564357757568,0.6447557210922241,0.6172666549682617,0.5916109085083008,0.5645398497581482,0.546058714389801,0.5237417221069336,0.5054600834846497,0.4898602366447449,0.4766682982444763,0.4654867351055145,0.4560147523880005,0.4520334005355835,0.4441031217575073,0.4394036531448364,0.43483150005340576,0.432275652885437,0.42810627818107605,0.42333847284317017,0.4198639690876007,0.41642236709594727,0.4129834473133087,0.40995168685913086,0.4085131883621216,0.40652474761009216,0.40422582626342773,0.40261322259902954,0.4012569785118103,0.40024322271347046,0.39856284856796265,0.39685460925102234,0.39470088481903076,0.39300569891929626,0.3889904320240021,0.3884829580783844,0.3880476653575897,0.3867228329181671,0.38630685210227966,0.38654395937919617,0.38534364104270935,0.3851052522659302,0.3844050467014313,0.3831738829612732,0.3827449083328247,0.3812449276447296,0.3791591227054596,0.3781805634498596,0.37711647152900696,0.377205491065979,0.37645018100738525,0.3757651448249817,0.3755682706832886,0.37524694204330444,0.37875688076019287,0.37732917070388794,0.37623724341392517,0.37457576394081116,0.37467673420906067,0.3724035620689392,0.3716093599796295,0.36956214904785156,0.36522454023361206,0.36617860198020935,0.3657819628715515,0.3679833710193634,0.36746665835380554,0.3699825704097748,0.36835169792175293,0.3673630952835083,0.36648961901664734,0.36530473828315735,0.36467722058296204,0.364077091217041,0.3637963533401489,0.3629908263683319,0.36457422375679016,0.35911089181900024,0.35895997285842896,0.35500115156173706,0.3551446497440338,0.3550451397895813,0.35580432415008545,0.3556227385997772,0.3563656210899353,0.3560236990451813,0.3558085858821869,0.3551061451435089,0.35677969455718994,0.35603761672973633,0.35525432229042053,0.35504910349845886,0.35511013865470886,0.3580488860607147,0.35886332392692566,0.3573910892009735,0.3550390601158142,0.35489386320114136,0.3543407618999481,0.3526039123535156,0.3528651297092438,0.35205572843551636,0.3511785864830017,0.3512871265411377,0.3514309227466583,0.3505800664424896,0.3498309254646301,0.3492546081542969,0.34937041997909546,0.34868937730789185,0.3483312726020813,0.34883788228034973,0.3491258919239044,0.34866735339164734,0.34868374466896057,0.3492286801338196,0.35131216049194336,0.34777453541755676,0.3485007882118225,0.34929558634757996,0.3492518663406372,0.3482206463813782,0.3491745889186859,0.3497646450996399,0.35040083527565,0.3498726189136505,0.3492090106010437,0.35153114795684814,0.35126060247421265,0.35072991251945496,0.3516625165939331,0.35261738300323486,0.35112905502319336,0.35023635625839233,0.34976041316986084,0.3543887436389923,0.3526272773742676,0.35197487473487854,0.3535937964916229,0.3482532203197479,0.3479084074497223,0.3475969433784485,0.3468228578567505,0.34670814871788025,0.3474016785621643,0.3475228548049927,0.34732717275619507,0.34302833676338196,0.3419494926929474,0.343720942735672,0.34348756074905396,0.3440922200679779,0.3443652093410492,0.3441040813922882,0.34405791759490967,0.3446459174156189,0.3463466465473175,0.34716594219207764,0.34752705693244934,0.34607765078544617,0.3461511731147766,0.34633705019950867,0.3475577235221863,0.35083791613578796,0.3500663638114929,0.34858238697052,0.3479965627193451,0.3478889763355255,0.34975501894950867,0.34914571046829224,0.35219261050224304,0.3512120842933655,0.34881213307380676,0.3481827676296234,0.34857305884361267,0.3513081967830658,0.34464842081069946,0.34286004304885864,0.34250926971435547,0.343431293964386,0.34457412362098694,0.3456340432167053,0.34782156348228455,0.34726592898368835,0.3453429043292999,0.3471817374229431,0.350191205739975,0.34902888536453247,0.3515470623970032,0.3480689823627472,0.34747934341430664,0.34733182191848755,0.34636420011520386,0.3455585837364197,0.34487614035606384,0.34523966908454895,0.3491594195365906,0.3483430743217468,0.3481292724609375,0.346477210521698,0.34713444113731384,0.3463717997074127,0.3435552418231964,0.34485864639282227,0.3420591354370117,0.34195971488952637,0.34193047881126404,0.34371206164360046,0.3442990183830261,0.342517614364624,0.3416523039340973,0.3412216603755951,0.3426712155342102,0.34856805205345154,0.3483165502548218,0.3481173515319824,0.3468376100063324,0.3466668426990509,0.345883309841156,0.346693217754364,0.346459299325943,0.3461534380912781,0.3460208773612976,0.34954866766929626,0.3526016175746918,0.3499552011489868,0.3492995500564575,0.34822842478752136,0.3477959632873535,0.3473627269268036,0.3473842144012451,0.3472473919391632,0.34750235080718994,0.3476363718509674,0.34735941886901855,0.3467637896537781,0.34627392888069153,0.3460130989551544,0.3464868664741516,0.34725913405418396,0.3470385670661926,0.3483504354953766,0.34736108779907227,0.3471669852733612,0.3473116457462311,0.34665510058403015,0.3459128141403198,0.34701889753341675,0.3512830436229706,0.3501286506652832,0.3479611575603485,0.34835922718048096,0.3465878665447235,0.3461855947971344,0.35029423236846924,0.34130460023880005,0.34140998125076294,0.34145548939704895,0.342166543006897,0.34022024273872375,0.34158846735954285,0.34336423873901367,0.34583520889282227,0.3464970290660858,0.3462299108505249,0.34753546118736267,0.3518421947956085,0.35282817482948303,0.35003888607025146,0.34955427050590515,0.34786197543144226,0.34652602672576904,0.3464994728565216,0.3465751111507416,0.34318894147872925,0.3434668779373169,0.34378620982170105,0.3441050350666046,0.344228595495224,0.34379905462265015,0.34283921122550964,0.34321102499961853,0.34310418367385864,0.3462978005409241,0.34753215312957764,0.3466610610485077,0.3463064730167389,0.3459947407245636,0.3467675447463989,0.34601688385009766,0.3464861214160919,0.34602898359298706,0.3473176658153534,0.3477018475532532,0.34818384051322937,0.348614364862442,0.35117748379707336,0.35511234402656555,0.35619544982910156,0.3528733253479004,0.3504848778247833],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"line\":{\"color\":\"#009e73\"},\"mode\":\"lines\",\"name\":\"train accuracy\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299],\"y\":[0.5068663954734802,0.5730336904525757,0.6629213690757751,0.7103620767593384,0.7315855026245117,0.7390761375427246,0.7390761375427246,0.7540574073791504,0.7665418386459351,0.7690386772155762,0.7665418386459351,0.7690386772155762,0.7652933597564697,0.7727839946746826,0.7777777910232544,0.7815231084823608,0.7827715277671814,0.7852684259414673,0.7865168452262878,0.7877652645111084,0.7890137434005737,0.7902621626853943,0.7915105819702148,0.7927590608596802,0.7940074801445007,0.7990012764930725,0.7977527976036072,0.8002496957778931,0.8002496957778931,0.8014981150627136,0.802746593952179,0.802746593952179,0.8064919114112854,0.8089887499809265,0.8089887499809265,0.8114856481552124,0.8114856481552124,0.8114856481552124,0.812734067440033,0.8114856481552124,0.8089887499809265,0.8089887499809265,0.8114856481552124,0.8114856481552124,0.8102372288703918,0.8114856481552124,0.8139825463294983,0.812734067440033,0.812734067440033,0.812734067440033,0.8152309656143188,0.8152309656143188,0.8164793848991394,0.8227216005325317,0.8202247023582458,0.8227216005325317,0.8227216005325317,0.8227216005325317,0.8214731812477112,0.8214731812477112,0.8227216005325317,0.8252184987068176,0.8214731812477112,0.8214731812477112,0.8252184987068176,0.8252184987068176,0.8252184987068176,0.8227216005325317,0.8239700198173523,0.8239700198173523,0.8239700198173523,0.8239700198173523,0.8227216005325317,0.8252184987068176,0.8264669179916382,0.8277153372764587,0.8252184987068176,0.8214731812477112,0.8264669179916382,0.8277153372764587,0.8277153372764587,0.8289638161659241,0.8302122354507446,0.8277153372764587,0.8277153372764587,0.8252184987068176,0.8289638161659241,0.8302122354507446,0.8302122354507446,0.8289638161659241,0.8289638161659241,0.8289638161659241,0.8314606547355652,0.8289638161659241,0.8314606547355652,0.8302122354507446,0.8302122354507446,0.8289638161659241,0.8264669179916382,0.8277153372764587,0.8264669179916382,0.8314606547355652,0.8339575529098511,0.8314606547355652,0.8264669179916382,0.8314606547355652,0.8289638161659241,0.8314606547355652,0.8327091336250305,0.8314606547355652,0.8327091336250305,0.8302122354507446,0.8314606547355652,0.8302122354507446,0.8327091336250305,0.8314606547355652,0.8289638161659241,0.8302122354507446,0.8327091336250305,0.8352059721946716,0.8302122354507446,0.8302122354507446,0.8327091336250305,0.8289638161659241,0.8314606547355652,0.8327091336250305,0.836454451084137,0.8339575529098511,0.8352059721946716,0.8352059721946716,0.8314606547355652,0.8352059721946716,0.836454451084137,0.8377028703689575,0.8339575529098511,0.8352059721946716,0.8339575529098511,0.8339575529098511,0.8339575529098511,0.8339575529098511,0.8327091336250305,0.8339575529098511,0.8339575529098511,0.836454451084137,0.8339575529098511,0.8314606547355652,0.8252184987068176,0.8239700198173523,0.8339575529098511,0.8314606547355652,0.8327091336250305,0.8339575529098511,0.836454451084137,0.836454451084137,0.8314606547355652,0.8352059721946716,0.8314606547355652,0.836454451084137,0.8377028703689575,0.8377028703689575,0.8389512896537781,0.8401997685432434,0.8389512896537781,0.8401997685432434,0.841448187828064,0.8401997685432434,0.8401997685432434,0.8389512896537781,0.8401997685432434,0.841448187828064,0.8377028703689575,0.8389512896537781,0.8401997685432434,0.8377028703689575,0.8389512896537781,0.8389512896537781,0.8401997685432434,0.8389512896537781,0.8389512896537781,0.8389512896537781,0.8401997685432434,0.841448187828064,0.8401997685432434,0.8352059721946716,0.836454451084137,0.8377028703689575,0.8377028703689575,0.8451935052871704,0.8451935052871704,0.8426966071128845,0.841448187828064,0.841448187828064,0.841448187828064,0.841448187828064,0.841448187828064,0.841448187828064,0.841448187828064,0.841448187828064,0.841448187828064,0.8426966071128845,0.8426966071128845,0.8439450860023499,0.8401997685432434,0.841448187828064,0.841448187828064,0.841448187828064,0.8426966071128845,0.841448187828064,0.8426966071128845,0.8401997685432434,0.8426966071128845,0.841448187828064,0.8401997685432434,0.841448187828064,0.8389512896537781,0.8401997685432434,0.8439450860023499,0.841448187828064,0.8377028703689575,0.8401997685432434,0.841448187828064,0.841448187828064,0.8439450860023499,0.846441924571991,0.8439450860023499,0.8439450860023499,0.8439450860023499,0.8439450860023499,0.8439450860023499,0.8426966071128845,0.8439450860023499,0.8451935052871704,0.846441924571991,0.8451935052871704,0.8426966071128845,0.8451935052871704,0.8439450860023499,0.8451935052871704,0.8451935052871704,0.8426966071128845,0.8439450860023499,0.8389512896537781,0.8451935052871704,0.8426966071128845,0.8439450860023499,0.8426966071128845,0.846441924571991,0.8439450860023499,0.846441924571991,0.8451935052871704,0.8451935052871704,0.8451935052871704,0.8476904034614563,0.8489388227462769,0.8451935052871704,0.8476904034614563,0.8476904034614563,0.8476904034614563,0.8451935052871704,0.8439450860023499,0.846441924571991,0.8451935052871704,0.846441924571991,0.8476904034614563,0.8501872420310974,0.8451935052871704,0.8476904034614563,0.846441924571991,0.846441924571991,0.8489388227462769,0.8489388227462769,0.8489388227462769,0.8514357209205627,0.8514357209205627,0.8526841402053833,0.8514357209205627,0.8514357209205627,0.8501872420310974,0.8501872420310974,0.8514357209205627,0.8476904034614563,0.846441924571991,0.8476904034614563,0.8476904034614563,0.8489388227462769,0.8489388227462769,0.8501872420310974,0.8501872420310974,0.8514357209205627,0.8476904034614563,0.8489388227462769,0.8489388227462769,0.8476904034614563,0.8476904034614563,0.8489388227462769,0.8489388227462769,0.8501872420310974,0.8489388227462769,0.8489388227462769,0.8489388227462769],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"line\":{\"color\":\"#f0e442\"},\"mode\":\"lines\",\"name\":\"validation accuracy\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299],\"y\":[0.5222222208976746,0.644444465637207,0.7111111283302307,0.7222222089767456,0.7333333492279053,0.7444444298744202,0.7777777910232544,0.8222222328186035,0.8222222328186035,0.8333333134651184,0.8111110925674438,0.8222222328186035,0.8333333134651184,0.8333333134651184,0.8444444537162781,0.8333333134651184,0.8222222328186035,0.8222222328186035,0.8111110925674438,0.8111110925674438,0.8111110925674438,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8111110925674438,0.8111110925674438,0.8111110925674438,0.8111110925674438,0.8111110925674438,0.8111110925674438,0.8111110925674438,0.8111110925674438,0.8111110925674438,0.8111110925674438,0.8111110925674438,0.8111110925674438,0.8111110925674438,0.8111110925674438,0.8111110925674438,0.8111110925674438,0.8222222328186035,0.8222222328186035,0.8111110925674438,0.8111110925674438,0.8111110925674438,0.8111110925674438,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8111110925674438,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8111110925674438,0.8111110925674438,0.8111110925674438,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8333333134651184,0.8333333134651184,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8333333134651184,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8333333134651184,0.8333333134651184,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8333333134651184,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8333333134651184,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8444444537162781,0.8444444537162781,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8222222328186035,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8222222328186035,0.8222222328186035,0.8333333134651184,0.8222222328186035,0.8333333134651184,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8333333134651184,0.8333333134651184,0.8222222328186035,0.8222222328186035,0.8222222328186035,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8444444537162781,0.8444444537162781,0.8444444537162781,0.8333333134651184,0.855555534362793,0.8444444537162781,0.8444444537162781,0.8444444537162781,0.8444444537162781,0.8333333134651184,0.8222222328186035,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8444444537162781,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8222222328186035,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8444444537162781,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184,0.8333333134651184],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.575,1.0]},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.0,1.0]},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,0.425]},\"title\":{\"text\":\"Training History\"},\"height\":768,\"width\":1024},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('5e07f8d5-5307-46fb-a7e1-7f2f784e257a');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure_history = make_subplots(rows=2, cols=1)\n",
    "\n",
    "x_range = list(range(1, len(fit.history[\"loss\"])))\n",
    "figure_history.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x_range,\n",
    "        y=fit.history[\"loss\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"train loss\",\n",
    "        line_color=CUD_COLORS[0],\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "figure_history.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x_range,\n",
    "        y=fit.history[\"val_loss\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"validation loss\",\n",
    "        line_color=CUD_COLORS[1],\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "figure_history.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x_range,\n",
    "        y=fit.history[\"accuracy\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"train accuracy\",\n",
    "        line_color=CUD_COLORS[2],\n",
    "    ),\n",
    "    row=2,\n",
    "    col=1,\n",
    ")\n",
    "figure_history.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x_range,\n",
    "        y=fit.history[\"val_accuracy\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"validation accuracy\",\n",
    "        line_color=CUD_COLORS[3],\n",
    "    ),\n",
    "    row=2,\n",
    "    col=1,\n",
    ")\n",
    "figure_history.update_layout(\n",
    "    height=768,\n",
    "    width=1024,\n",
    "    title_text=\"Training History\",\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "figure_history.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6fc9bf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T08:48:54.294116Z",
     "iopub.status.busy": "2022-09-27T08:48:54.293791Z",
     "iopub.status.idle": "2022-09-27T08:48:54.404524Z",
     "shell.execute_reply": "2022-09-27T08:48:54.403634Z"
    },
    "papermill": {
     "duration": 0.162624,
     "end_time": "2022-09-27T08:48:54.406676",
     "exception": false,
     "start_time": "2022-09-27T08:48:54.244052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test).round()\n",
    "results = pd.DataFrame(passenger_ids_test.copy())\n",
    "results[\"Survived\"] = prediction.astype(int)\n",
    "results.to_csv(\"results.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 47.323381,
   "end_time": "2022-09-27T08:48:57.309476",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-09-27T08:48:09.986095",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
